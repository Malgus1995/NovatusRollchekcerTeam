{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5b02687e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "cbb47399",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = './Data preprocessed/CSM1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0d987f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_list = os.listdir('./Data preprocessed/CSM1')\n",
    "practice_folder_list=[]\n",
    "\n",
    "for fd in folder_list:\n",
    "    if '9' in fd:\n",
    "        practice_folder_list.append(fd)\n",
    "practice_folder_list =[os.path.join(base_path,pfl) for pfl in practice_folder_list]\n",
    "\n",
    "true_img_list = [os.path.join(practice_folder_list[0],tip) for tip in os.listdir(practice_folder_list[0])]\n",
    "false_img_list = [os.path.join(practice_folder_list[1],tip) for tip in os.listdir(practice_folder_list[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c62d7b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "zzz =cv2.imread(true_img_list[0],cv2.IMREAD_GRAYSCALE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ed1617af",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs = np.zeros((len(true_img_list),1,28,28))\n",
    "\n",
    "for idx,img in enumerate(true_img_list):\n",
    "    tmp_grayscale_img = cv2.imread(img,cv2.IMREAD_GRAYSCALE)\n",
    "    tmp_grayscale_img = cv2.resize(tmp_grayscale_img,(28,28))\n",
    "    train_imgs[idx,0,:,:] = tmp_grayscale_img\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5275d601",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "88dbf837",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = 4\n",
    "stride = 1\n",
    "padding = 0\n",
    "init_kernel = 16\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    " \n",
    "        # encoder\n",
    "        self.enc1 = nn.Conv2d(\n",
    "            in_channels=1, out_channels=init_kernel, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.enc2 = nn.Conv2d(\n",
    "            in_channels=init_kernel, out_channels=init_kernel*2, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.enc3 = nn.Conv2d(\n",
    "            in_channels=init_kernel*2, out_channels=init_kernel*4, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.enc4 = nn.Conv2d(\n",
    "            in_channels=init_kernel*4, out_channels=init_kernel*8, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.enc5 = nn.Conv2d(\n",
    "            in_channels=init_kernel*8, out_channels=init_kernel, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "\n",
    "        # decoder \n",
    "        self.dec1 = nn.ConvTranspose2d(\n",
    "            in_channels=init_kernel, out_channels=init_kernel*8, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.dec2 = nn.ConvTranspose2d(\n",
    "            in_channels=init_kernel*8, out_channels=init_kernel*4, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.dec3 = nn.ConvTranspose2d(\n",
    "            in_channels=init_kernel*4, out_channels=init_kernel*2, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.dec4 = nn.ConvTranspose2d(\n",
    "            in_channels=init_kernel*2, out_channels=init_kernel, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.dec5 = nn.ConvTranspose2d(\n",
    "            in_channels=init_kernel, out_channels=1, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\"\n",
    "        :param mu: mean from the encoder's latent space\n",
    "        :param log_var: log variance from the encoder's latent space\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5*log_var) # standard deviation\n",
    "        eps = torch.randn_like(std) # `randn_like` as we need the same size\n",
    "        sample = mu + (eps * std) # sampling\n",
    "        return sample\n",
    " \n",
    "    def forward(self, x):\n",
    "        # encoding\n",
    "        x = F.relu(self.enc1(x))\n",
    "        x = F.relu(self.enc2(x))\n",
    "        x = F.relu(self.enc3(x))\n",
    "        x = F.relu(self.enc4(x))\n",
    "        x = self.enc5(x)\n",
    "\n",
    "        # get `mu` and `log_var`\n",
    "        mu = x\n",
    "        log_var = x\n",
    "\n",
    "        # get the latent vector through reparameterization\n",
    "        z = self.reparameterize(mu, log_var)\n",
    " \n",
    "        # decoding\n",
    "        x = F.relu(self.dec1(z))\n",
    "        x = F.relu(self.dec2(x))\n",
    "        x = F.relu(self.dec3(x))\n",
    "        x = F.relu(self.dec4(x))\n",
    "        reconstruction = torch.sigmoid(self.dec5(x))\n",
    "        return reconstruction, mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "60a987c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE()\n",
    "optimizer = torch.optim.Adam(params=vae.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "def vae_loss(recon_x,x,mu,log_var):\n",
    "    \n",
    "    def gaussian_likelihood(x_hat,x):\n",
    "        log_scale = nn.Parameter(torch.Tensor([0.0]))\n",
    "        scale=  torch.exp(torch.tensor(log_scale))\n",
    "        mean = x_hat\n",
    "        dist = torch.distributions.Normal(mean,scale)\n",
    "        \n",
    "        log_pxz = dist.log_prob(x)\n",
    "        \n",
    "        return log_pxz.sum(dim=(1))\n",
    "    \n",
    "    \n",
    "    recon_loss = gaussian_likelihood(recon_x.view(-1,1,28,28), x.view(-1, 1,28,28))\n",
    "    kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return (recon_loss + kl_loss).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "355ff9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, dataloader,epochs):\n",
    "    criterion = nn.MSELoss()\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for epoch in range(0,epochs): \n",
    "        print(f\"Epoch {epoch+1}\")\n",
    "        for i,data in enumerate(dataloader): \n",
    "            optimizer.zero_grad()\n",
    "            data =  torch.tensor(data, dtype = torch.float32)\n",
    "            reconstruction, mu, logvar = model(data)\n",
    "            loss = criterion(reconstruction,data)\n",
    "            #loss = vae_loss(reconstruction,data, mu, logvar)\n",
    "            loss.backward()\n",
    "            running_loss += loss.item()\n",
    "            optimizer.step()\n",
    "        train_loss = running_loss/len(dataloader.dataset)\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "8a030bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_imgs, batch_size=16,\n",
    "                                         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3392dddf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "c46d0e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cd/hq134zy930x005dbg266m4wh0000gn/T/ipykernel_19302/978004405.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data =  torch.tensor(data, dtype = torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4130.9732\n",
      "Epoch 2\n",
      "Train Loss: 8261.4095\n",
      "Epoch 3\n",
      "Train Loss: 12391.6574\n",
      "Epoch 4\n",
      "Train Loss: 16521.5664\n",
      "Epoch 5\n",
      "Train Loss: 20648.6152\n",
      "Epoch 6\n",
      "Train Loss: 24768.2149\n",
      "Epoch 7\n",
      "Train Loss: 28884.2233\n",
      "Epoch 8\n",
      "Train Loss: 32999.8845\n",
      "Epoch 9\n",
      "Train Loss: 37115.3244\n",
      "Epoch 10\n",
      "Train Loss: 41230.8005\n",
      "Epoch 11\n",
      "Train Loss: 45346.2623\n",
      "Epoch 12\n",
      "Train Loss: 49461.6906\n",
      "Epoch 13\n",
      "Train Loss: 53577.0811\n",
      "Epoch 14\n",
      "Train Loss: 57692.4179\n",
      "Epoch 15\n",
      "Train Loss: 61807.8405\n",
      "Epoch 16\n",
      "Train Loss: 65923.2716\n",
      "Epoch 17\n",
      "Train Loss: 70038.6944\n",
      "Epoch 18\n",
      "Train Loss: 74154.0525\n",
      "Epoch 19\n",
      "Train Loss: 78269.4580\n",
      "Epoch 20\n",
      "Train Loss: 82384.8647\n",
      "Epoch 21\n",
      "Train Loss: 86500.2726\n",
      "Epoch 22\n",
      "Train Loss: 90615.6863\n",
      "Epoch 23\n",
      "Train Loss: 94731.1537\n",
      "Epoch 24\n",
      "Train Loss: 98846.4814\n",
      "Epoch 25\n",
      "Train Loss: 102961.8890\n",
      "Epoch 26\n",
      "Train Loss: 107077.3907\n",
      "Epoch 27\n",
      "Train Loss: 111192.7570\n",
      "Epoch 28\n",
      "Train Loss: 115308.1479\n",
      "Epoch 29\n",
      "Train Loss: 119423.4737\n",
      "Epoch 30\n",
      "Train Loss: 123538.9044\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "123538.90442288306"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(vae,train_dataloader,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "5849eb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = vae.forward(torch.tensor(train_imgs[0], dtype = torch.float32).view((1,1,28,28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "b8d02255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 28, 28])"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "fbd0948a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9113895940>"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(res[0][0][0].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "efa7650a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5435096 , 0.54424405, 0.5416362 , 0.54166144, 0.5412116 ,\n",
       "        0.5402875 , 0.540158  , 0.54004663, 0.53969944, 0.53977025,\n",
       "        0.5393055 , 0.5397438 , 0.54036826, 0.5394429 , 0.53935385,\n",
       "        0.54054826, 0.53917557, 0.53932345, 0.53965724, 0.5401792 ,\n",
       "        0.5395255 , 0.54006624, 0.5400837 , 0.53969604, 0.5395899 ,\n",
       "        0.53168577, 0.5309222 , 0.5350675 ],\n",
       "       [0.54186296, 0.54351306, 0.5388431 , 0.533679  , 0.5330992 ,\n",
       "        0.53157604, 0.5323408 , 0.53311795, 0.5319965 , 0.53183097,\n",
       "        0.53200364, 0.53114766, 0.5312144 , 0.5308093 , 0.5319927 ,\n",
       "        0.5307521 , 0.5305645 , 0.5292045 , 0.5284906 , 0.530959  ,\n",
       "        0.5302365 , 0.52984965, 0.5324442 , 0.5305148 , 0.52955383,\n",
       "        0.5232734 , 0.52198523, 0.52939326],\n",
       "       [0.5399789 , 0.5396582 , 0.5391527 , 0.53586113, 0.53559315,\n",
       "        0.53336674, 0.5338386 , 0.53183174, 0.53167474, 0.5275618 ,\n",
       "        0.5248268 , 0.5212505 , 0.5178232 , 0.51989573, 0.5186136 ,\n",
       "        0.5155189 , 0.51585114, 0.5100918 , 0.51318264, 0.5147393 ,\n",
       "        0.5181379 , 0.52298677, 0.5283394 , 0.5275286 , 0.52723485,\n",
       "        0.5259873 , 0.52717096, 0.5318919 ],\n",
       "       [0.54624057, 0.54403126, 0.53938746, 0.53035015, 0.53025424,\n",
       "        0.52474374, 0.51545495, 0.5107361 , 0.5009038 , 0.49273533,\n",
       "        0.48486245, 0.47740036, 0.46688834, 0.46167693, 0.46055582,\n",
       "        0.45525596, 0.44143832, 0.445322  , 0.4527784 , 0.45572516,\n",
       "        0.46523172, 0.4857743 , 0.49925023, 0.51234925, 0.51733035,\n",
       "        0.5158066 , 0.519718  , 0.5284652 ],\n",
       "       [0.5449351 , 0.5445956 , 0.54116213, 0.5274006 , 0.5211669 ,\n",
       "        0.51442784, 0.5033544 , 0.4870243 , 0.4757615 , 0.45852438,\n",
       "        0.4419249 , 0.43868503, 0.41405517, 0.40000764, 0.3963748 ,\n",
       "        0.40004402, 0.3802448 , 0.37783396, 0.38193104, 0.38159615,\n",
       "        0.40331048, 0.42976156, 0.46648616, 0.48650882, 0.50318795,\n",
       "        0.50905263, 0.51622516, 0.52818805],\n",
       "       [0.544803  , 0.54423165, 0.5412419 , 0.5298625 , 0.5228091 ,\n",
       "        0.50765944, 0.4897339 , 0.4688371 , 0.45011896, 0.4350308 ,\n",
       "        0.41316792, 0.40030515, 0.38913947, 0.36420193, 0.3469626 ,\n",
       "        0.34120882, 0.32072505, 0.32596725, 0.32655922, 0.34967512,\n",
       "        0.35748193, 0.38222533, 0.434565  , 0.4730618 , 0.4964632 ,\n",
       "        0.5039182 , 0.5154768 , 0.52780133],\n",
       "       [0.5449337 , 0.5438258 , 0.5386177 , 0.5255954 , 0.5183696 ,\n",
       "        0.50275695, 0.47895765, 0.4515235 , 0.4212056 , 0.40835944,\n",
       "        0.39513573, 0.3838203 , 0.37359062, 0.35465652, 0.34546983,\n",
       "        0.32503024, 0.31529725, 0.29797697, 0.3042122 , 0.3172897 ,\n",
       "        0.3426596 , 0.36815402, 0.40742213, 0.45006385, 0.48330283,\n",
       "        0.5009358 , 0.5142016 , 0.52758163],\n",
       "       [0.54490006, 0.5437613 , 0.53886575, 0.52285725, 0.51303995,\n",
       "        0.50012094, 0.4779913 , 0.4452705 , 0.4184534 , 0.3948333 ,\n",
       "        0.38114303, 0.3659489 , 0.36654142, 0.35834667, 0.34965074,\n",
       "        0.32320875, 0.31233278, 0.29512468, 0.2992442 , 0.30231667,\n",
       "        0.32199594, 0.3657245 , 0.3954671 , 0.44452575, 0.4712791 ,\n",
       "        0.4960125 , 0.5126991 , 0.5271845 ],\n",
       "       [0.5447791 , 0.5429959 , 0.53932714, 0.52168715, 0.51335925,\n",
       "        0.49797654, 0.46938205, 0.4360998 , 0.39972138, 0.38832495,\n",
       "        0.37902054, 0.37464824, 0.37216803, 0.35005623, 0.34837535,\n",
       "        0.32280788, 0.31385523, 0.3119838 , 0.3169331 , 0.32185882,\n",
       "        0.3343129 , 0.35096157, 0.39253822, 0.43600324, 0.46898735,\n",
       "        0.4922973 , 0.5125281 , 0.5270196 ],\n",
       "       [0.54473305, 0.5432101 , 0.5388746 , 0.52136326, 0.5097053 ,\n",
       "        0.48659524, 0.4639648 , 0.4408453 , 0.40538815, 0.38120747,\n",
       "        0.36668   , 0.36499503, 0.3741724 , 0.37167603, 0.37220216,\n",
       "        0.35842994, 0.3540242 , 0.33356735, 0.33624497, 0.35994583,\n",
       "        0.34950286, 0.37474662, 0.39491138, 0.4374606 , 0.46694785,\n",
       "        0.49074233, 0.5108185 , 0.52689934],\n",
       "       [0.54486203, 0.5440964 , 0.53856486, 0.51824546, 0.50798637,\n",
       "        0.49249047, 0.46626037, 0.4292615 , 0.41121426, 0.3954976 ,\n",
       "        0.39211625, 0.38631046, 0.38468146, 0.3961808 , 0.3860103 ,\n",
       "        0.38367152, 0.36294597, 0.3641017 , 0.3585243 , 0.3568764 ,\n",
       "        0.37199706, 0.38051912, 0.39695865, 0.43303606, 0.46510157,\n",
       "        0.48982388, 0.50942755, 0.52633524],\n",
       "       [0.54467916, 0.5430583 , 0.53979653, 0.520504  , 0.5062928 ,\n",
       "        0.4918012 , 0.46869734, 0.4473723 , 0.40272397, 0.3958925 ,\n",
       "        0.3840782 , 0.396915  , 0.39639834, 0.3963532 , 0.39817324,\n",
       "        0.3969276 , 0.40114626, 0.39556867, 0.3804848 , 0.36620334,\n",
       "        0.38489056, 0.38554004, 0.402954  , 0.43381605, 0.46033382,\n",
       "        0.48994535, 0.51001596, 0.5260901 ],\n",
       "       [0.5449067 , 0.54277956, 0.53747207, 0.52042884, 0.50620997,\n",
       "        0.48713875, 0.47591805, 0.45171148, 0.42122942, 0.40490204,\n",
       "        0.40125078, 0.40039858, 0.40297732, 0.42163792, 0.42019367,\n",
       "        0.41812202, 0.4152022 , 0.39670447, 0.39479202, 0.3854839 ,\n",
       "        0.37809297, 0.38149434, 0.40968314, 0.43252233, 0.4603977 ,\n",
       "        0.4855238 , 0.5068919 , 0.52571654],\n",
       "       [0.5447948 , 0.5442302 , 0.5381643 , 0.5188722 , 0.507758  ,\n",
       "        0.48960248, 0.48233655, 0.45617405, 0.43241817, 0.41942513,\n",
       "        0.42684972, 0.42731497, 0.42451388, 0.43166807, 0.42727247,\n",
       "        0.423496  , 0.4261988 , 0.407237  , 0.3911057 , 0.38857436,\n",
       "        0.38125592, 0.3917096 , 0.40484405, 0.42180508, 0.45873776,\n",
       "        0.48257005, 0.50625694, 0.52521104],\n",
       "       [0.5447561 , 0.54333967, 0.5390947 , 0.52159655, 0.5066219 ,\n",
       "        0.49394315, 0.4748979 , 0.4548551 , 0.44184867, 0.42742303,\n",
       "        0.42263728, 0.42866355, 0.42848104, 0.43478882, 0.42879975,\n",
       "        0.42567688, 0.4322112 , 0.40801233, 0.39425698, 0.3793692 ,\n",
       "        0.37937734, 0.3821203 , 0.40153018, 0.4297983 , 0.44923586,\n",
       "        0.47892883, 0.50641805, 0.5255394 ],\n",
       "       [0.5448863 , 0.54310095, 0.537345  , 0.5206086 , 0.50913626,\n",
       "        0.49608037, 0.47821322, 0.4661132 , 0.44504353, 0.42939186,\n",
       "        0.42348453, 0.43899307, 0.42543796, 0.4395888 , 0.43472067,\n",
       "        0.43064368, 0.42238668, 0.41520154, 0.3974202 , 0.38208887,\n",
       "        0.36762252, 0.3661907 , 0.395227  , 0.4231712 , 0.45293215,\n",
       "        0.48147073, 0.504849  , 0.5253953 ],\n",
       "       [0.5444685 , 0.5438972 , 0.5391222 , 0.52006036, 0.5097463 ,\n",
       "        0.49449447, 0.47511116, 0.44834602, 0.43775496, 0.4230273 ,\n",
       "        0.42238006, 0.4187981 , 0.4280115 , 0.42712972, 0.43332827,\n",
       "        0.42398477, 0.42012498, 0.41297022, 0.4034269 , 0.37569228,\n",
       "        0.37028036, 0.378616  , 0.39323494, 0.4291338 , 0.45074838,\n",
       "        0.4782657 , 0.5058111 , 0.5259366 ],\n",
       "       [0.5448403 , 0.54417354, 0.5381387 , 0.52327204, 0.5132565 ,\n",
       "        0.49691635, 0.47851384, 0.4358137 , 0.42411774, 0.40458977,\n",
       "        0.39942762, 0.39446145, 0.403161  , 0.413489  , 0.40855435,\n",
       "        0.40965888, 0.41126755, 0.4003448 , 0.38397956, 0.37804034,\n",
       "        0.39500892, 0.3840477 , 0.4018772 , 0.4312869 , 0.45519266,\n",
       "        0.4839737 , 0.5065843 , 0.5258414 ],\n",
       "       [0.5445652 , 0.5425543 , 0.5379004 , 0.5212855 , 0.5144993 ,\n",
       "        0.49926698, 0.47129798, 0.4494913 , 0.42075264, 0.39195594,\n",
       "        0.38518277, 0.40258798, 0.392926  , 0.40094215, 0.4042525 ,\n",
       "        0.4092257 , 0.40410906, 0.39073783, 0.39285478, 0.3956539 ,\n",
       "        0.39330813, 0.4115728 , 0.42413774, 0.43398473, 0.46065384,\n",
       "        0.48670557, 0.5081723 , 0.5257992 ],\n",
       "       [0.54455245, 0.5432176 , 0.5387305 , 0.52190953, 0.5126923 ,\n",
       "        0.49529845, 0.47617903, 0.45918307, 0.42452207, 0.40818208,\n",
       "        0.39848226, 0.39756727, 0.39585868, 0.40224636, 0.40443164,\n",
       "        0.41358936, 0.40550855, 0.40507588, 0.40921104, 0.41895697,\n",
       "        0.40723592, 0.43068638, 0.43311107, 0.45113927, 0.46740177,\n",
       "        0.48865354, 0.5100343 , 0.52637815],\n",
       "       [0.5452021 , 0.543326  , 0.53766614, 0.5223491 , 0.5108086 ,\n",
       "        0.50205225, 0.48083985, 0.46499354, 0.4497469 , 0.42559505,\n",
       "        0.41130596, 0.40141743, 0.38807955, 0.41197318, 0.42463747,\n",
       "        0.4199353 , 0.41902754, 0.42285404, 0.43069682, 0.42938438,\n",
       "        0.44702294, 0.43968406, 0.4519398 , 0.46904808, 0.48204988,\n",
       "        0.4967697 , 0.5109067 , 0.52646697],\n",
       "       [0.54482704, 0.5433347 , 0.537609  , 0.52255356, 0.5166518 ,\n",
       "        0.5034602 , 0.4972013 , 0.48850098, 0.46095943, 0.44474718,\n",
       "        0.43232906, 0.42928085, 0.43539742, 0.44138944, 0.43973973,\n",
       "        0.4450188 , 0.44061926, 0.44206318, 0.45733318, 0.47057146,\n",
       "        0.46795446, 0.4757284 , 0.4797906 , 0.48098063, 0.49350512,\n",
       "        0.49865726, 0.51334244, 0.5269661 ],\n",
       "       [0.5444802 , 0.5436757 , 0.5375461 , 0.5239292 , 0.51661724,\n",
       "        0.5134097 , 0.50921243, 0.50084955, 0.49438024, 0.47568852,\n",
       "        0.46608695, 0.4665637 , 0.46250802, 0.47277412, 0.46462926,\n",
       "        0.4850114 , 0.4770459 , 0.4746755 , 0.47598386, 0.48463342,\n",
       "        0.49242866, 0.4950826 , 0.49514955, 0.49859738, 0.50433093,\n",
       "        0.50584185, 0.51538163, 0.52736044],\n",
       "       [0.5452198 , 0.5431193 , 0.5368597 , 0.5217397 , 0.5168669 ,\n",
       "        0.51497835, 0.50415766, 0.5044179 , 0.50116765, 0.49151364,\n",
       "        0.49157286, 0.4847213 , 0.48523912, 0.4812972 , 0.48341936,\n",
       "        0.49178588, 0.48954123, 0.49337253, 0.48972195, 0.49437457,\n",
       "        0.5013755 , 0.50311315, 0.5064979 , 0.5068148 , 0.5108763 ,\n",
       "        0.5113897 , 0.51847196, 0.5281714 ],\n",
       "       [0.54476184, 0.5422155 , 0.5342424 , 0.5235272 , 0.51996   ,\n",
       "        0.51400024, 0.51649785, 0.5163804 , 0.510184  , 0.50555956,\n",
       "        0.49975348, 0.4974551 , 0.50451124, 0.50371385, 0.5032211 ,\n",
       "        0.5035399 , 0.504817  , 0.504421  , 0.5074094 , 0.5077176 ,\n",
       "        0.51618356, 0.5140849 , 0.51547474, 0.5173604 , 0.51785713,\n",
       "        0.514665  , 0.5193549 , 0.5284291 ],\n",
       "       [0.53693575, 0.5329803 , 0.5288874 , 0.5213274 , 0.51901335,\n",
       "        0.5182526 , 0.5155141 , 0.5119329 , 0.51212996, 0.512915  ,\n",
       "        0.5090092 , 0.5094076 , 0.5090229 , 0.50691   , 0.5113773 ,\n",
       "        0.5108992 , 0.5113575 , 0.51296383, 0.5132755 , 0.512888  ,\n",
       "        0.5162889 , 0.5153892 , 0.51580113, 0.5158663 , 0.51688653,\n",
       "        0.5181919 , 0.52384466, 0.52921754],\n",
       "       [0.5390357 , 0.53370255, 0.5313204 , 0.5304548 , 0.5298498 ,\n",
       "        0.5299593 , 0.52876   , 0.5278057 , 0.5269506 , 0.5250489 ,\n",
       "        0.52549726, 0.52565426, 0.5258663 , 0.52587324, 0.52588546,\n",
       "        0.5255453 , 0.5262632 , 0.52752   , 0.52809143, 0.5286561 ,\n",
       "        0.52899194, 0.52942383, 0.529542  , 0.53024256, 0.53253347,\n",
       "        0.52952623, 0.5336709 , 0.53436357],\n",
       "       [0.5416637 , 0.53898984, 0.53434813, 0.5305192 , 0.5298189 ,\n",
       "        0.5294554 , 0.52882195, 0.52832335, 0.5284943 , 0.529006  ,\n",
       "        0.5285513 , 0.5282244 , 0.5282221 , 0.52788526, 0.52810836,\n",
       "        0.5285156 , 0.5290332 , 0.528582  , 0.529142  , 0.5289262 ,\n",
       "        0.5288738 , 0.52941084, 0.5292111 , 0.5299976 , 0.53142065,\n",
       "        0.52609354, 0.5285505 , 0.53216064]], dtype=float32)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0][0][0].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f0ca64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ab399e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7948c229",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
