{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4241862a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "47ec13fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = './Data preprocessed/CSM1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "78a60f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_list = os.listdir('./Data preprocessed/CSM1')\n",
    "practice_folder_list=[]\n",
    "\n",
    "for fd in folder_list:\n",
    "    if '9' in fd:\n",
    "        practice_folder_list.append(fd)\n",
    "practice_folder_list =[os.path.join(base_path,pfl) for pfl in practice_folder_list]\n",
    "\n",
    "true_img_list = [os.path.join(practice_folder_list[0],tip) for tip in os.listdir(practice_folder_list[0])]\n",
    "false_img_list = [os.path.join(practice_folder_list[1],tip) for tip in os.listdir(practice_folder_list[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1ae814dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "zzz =cv2.imread(true_img_list[0],cv2.IMREAD_GRAYSCALE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "756d3a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs = np.zeros((len(true_img_list),1,300,300))\n",
    "\n",
    "for idx,img in enumerate(true_img_list):\n",
    "    tmp_grayscale_img = cv2.imread(img,cv2.IMREAD_GRAYSCALE)\n",
    "    tmp_grayscale_img = cv2.resize(tmp_grayscale_img,(300,300))\n",
    "    train_imgs[idx,0,:,:] = tmp_grayscale_img\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ecbbd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "64c5fefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = 4\n",
    "stride = 1\n",
    "padding = 0\n",
    "init_kernel = 16\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    " \n",
    "        # encoder\n",
    "        self.enc1 = nn.Conv2d(\n",
    "            in_channels=1, out_channels=init_kernel, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.enc2 = nn.Conv2d(\n",
    "            in_channels=init_kernel, out_channels=init_kernel*2, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.enc3 = nn.Conv2d(\n",
    "            in_channels=init_kernel*2, out_channels=init_kernel*4, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.enc4 = nn.Conv2d(\n",
    "            in_channels=init_kernel*4, out_channels=init_kernel*8, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.enc5 = nn.Conv2d(\n",
    "            in_channels=init_kernel*8, out_channels=init_kernel, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "\n",
    "        # decoder \n",
    "        self.dec1 = nn.ConvTranspose2d(\n",
    "            in_channels=init_kernel, out_channels=init_kernel*8, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.dec2 = nn.ConvTranspose2d(\n",
    "            in_channels=init_kernel*8, out_channels=init_kernel*4, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.dec3 = nn.ConvTranspose2d(\n",
    "            in_channels=init_kernel*4, out_channels=init_kernel*2, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.dec4 = nn.ConvTranspose2d(\n",
    "            in_channels=init_kernel*2, out_channels=init_kernel, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.dec5 = nn.ConvTranspose2d(\n",
    "            in_channels=init_kernel, out_channels=1, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\"\n",
    "        :param mu: mean from the encoder's latent space\n",
    "        :param log_var: log variance from the encoder's latent space\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5*log_var) # standard deviation\n",
    "        eps = torch.randn_like(std) # `randn_like` as we need the same size\n",
    "        sample = mu + (eps * std) # sampling\n",
    "        return sample\n",
    " \n",
    "    def forward(self, x):\n",
    "        # encoding\n",
    "        x = F.relu(self.enc1(x))\n",
    "        x = F.relu(self.enc2(x))\n",
    "        x = F.relu(self.enc3(x))\n",
    "        x = F.relu(self.enc4(x))\n",
    "        x = self.enc5(x)\n",
    "\n",
    "        # get `mu` and `log_var`\n",
    "        mu = x\n",
    "        log_var = x\n",
    "\n",
    "        # get the latent vector through reparameterization\n",
    "        z = self.reparameterize(mu, log_var)\n",
    " \n",
    "        # decoding\n",
    "        x = F.relu(self.dec1(z))\n",
    "        x = F.relu(self.dec2(x))\n",
    "        x = F.relu(self.dec3(x))\n",
    "        x = F.relu(self.dec4(x))\n",
    "        reconstruction = torch.sigmoid(self.dec5(x))\n",
    "        return reconstruction, mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "48d273b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE()\n",
    "optimizer = torch.optim.Adam(params=vae.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "def vae_loss(recon_x,x,mu,log_var):\n",
    "    recon_loss = F.binary_cross_entropy(recon_x.view(-1,1,300,300), x.view(-1, 1,300,300), reduction='sum')\n",
    "    kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return recon_loss + kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5407f251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, dataloader,epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for epoch in range(0,epochs): \n",
    "        print(f\"Epoch {epoch+1}\")\n",
    "        for i,data in enumerate(dataloader): \n",
    "            optimizer.zero_grad()\n",
    "            data =  torch.tensor(data, dtype = torch.float32)\n",
    "            reconstruction, mu, logvar = model(data)\n",
    "            loss = vae_loss(reconstruction,data, mu, logvar)\n",
    "            loss.backward()\n",
    "            running_loss += loss.item()\n",
    "            optimizer.step()\n",
    "        train_loss = running_loss/len(dataloader.dataset)\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9684e95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_imgs, batch_size=16,\n",
    "                                         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8b0436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cd/hq134zy930x005dbg266m4wh0000gn/T/ipykernel_19302/1353311346.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data =  torch.tensor(data, dtype = torch.float32)\n"
     ]
    }
   ],
   "source": [
    "fit(vae,train_dataloader,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887b51d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = vae.forward(torch.tentrain_imgs[0]torch.tensor(data, dtype = torch.float32).view((1,1,28,28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133e22b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66ec6c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0467c7c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
