{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b5ec866d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7c14f5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = './Data preprocessed/CSM1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "92963e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_list = os.listdir('./Data preprocessed/CSM1')\n",
    "practice_folder_list=[]\n",
    "\n",
    "for fd in folder_list:\n",
    "    if '9' in fd:\n",
    "        practice_folder_list.append(fd)\n",
    "practice_folder_list =[os.path.join(base_path,pfl) for pfl in practice_folder_list]\n",
    "\n",
    "true_img_list = [os.path.join(practice_folder_list[0],tip) for tip in os.listdir(practice_folder_list[0])]\n",
    "false_img_list = [os.path.join(practice_folder_list[1],tip) for tip in os.listdir(practice_folder_list[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "79f03857",
   "metadata": {},
   "outputs": [],
   "source": [
    "zzz =cv2.imread(true_img_list[0],cv2.IMREAD_GRAYSCALE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e2c06a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs = np.zeros((len(true_img_list),1,28,28))\n",
    "\n",
    "for idx,img in enumerate(true_img_list):\n",
    "    tmp_grayscale_img = cv2.imread(img,cv2.IMREAD_GRAYSCALE)\n",
    "    tmp_grayscale_img = cv2.resize(tmp_grayscale_img,(28,28))\n",
    "    train_imgs[idx,0,:,:] = tmp_grayscale_img\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d566f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "232c13c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = 4\n",
    "stride = 1\n",
    "padding = 0\n",
    "init_kernel = 16\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    " \n",
    "        # encoder\n",
    "        self.enc1 = nn.Conv2d(\n",
    "            in_channels=1, out_channels=init_kernel, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.enc2 = nn.Conv2d(\n",
    "            in_channels=init_kernel, out_channels=init_kernel*2, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.enc3 = nn.Conv2d(\n",
    "            in_channels=init_kernel*2, out_channels=init_kernel*4, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.enc4 = nn.Conv2d(\n",
    "            in_channels=init_kernel*4, out_channels=init_kernel*8, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.enc5 = nn.Conv2d(\n",
    "            in_channels=init_kernel*8, out_channels=init_kernel, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "\n",
    "        # decoder \n",
    "        self.dec1 = nn.ConvTranspose2d(\n",
    "            in_channels=init_kernel, out_channels=init_kernel*8, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.dec2 = nn.ConvTranspose2d(\n",
    "            in_channels=init_kernel*8, out_channels=init_kernel*4, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.dec3 = nn.ConvTranspose2d(\n",
    "            in_channels=init_kernel*4, out_channels=init_kernel*2, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.dec4 = nn.ConvTranspose2d(\n",
    "            in_channels=init_kernel*2, out_channels=init_kernel, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "        self.dec5 = nn.ConvTranspose2d(\n",
    "            in_channels=init_kernel, out_channels=1, kernel_size=kernel_size, \n",
    "            stride=stride, padding=padding\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\"\n",
    "        :param mu: mean from the encoder's latent space\n",
    "        :param log_var: log variance from the encoder's latent space\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5*log_var) # standard deviation\n",
    "        eps = torch.randn_like(std) # `randn_like` as we need the same size\n",
    "        sample = mu + (eps * std) # sampling\n",
    "        return sample\n",
    " \n",
    "    def forward(self, x):\n",
    "        # encoding\n",
    "        x = F.relu(self.enc1(x))\n",
    "        x = F.relu(self.enc2(x))\n",
    "        x = F.relu(self.enc3(x))\n",
    "        x = F.relu(self.enc4(x))\n",
    "        x = self.enc5(x)\n",
    "\n",
    "        # get `mu` and `log_var`\n",
    "        mu = x\n",
    "        log_var = x\n",
    "\n",
    "        # get the latent vector through reparameterization\n",
    "        z = self.reparameterize(mu, log_var)\n",
    " \n",
    "        # decoding\n",
    "        x = F.relu(self.dec1(z))\n",
    "        x = F.relu(self.dec2(x))\n",
    "        x = F.relu(self.dec3(x))\n",
    "        x = F.relu(self.dec4(x))\n",
    "        reconstruction = torch.sigmoid(self.dec5(x))\n",
    "        return reconstruction, mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "b11185c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE()\n",
    "optimizer = torch.optim.Adam(params=vae.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "def vae_loss(recon_x,x,mu,log_var):\n",
    "    \n",
    "    def gaussian_likelihood(x_hat,x):\n",
    "        log_scale = nn.Parameter(torch.Tensor([0.0]))\n",
    "        scale=  torch.exp(torch.tensor(log_scale))\n",
    "        mean = x_hat\n",
    "        dist = torch.distributions.Normal(mean,scale)\n",
    "        \n",
    "        log_pxz = dist.log_prob(x)\n",
    "        \n",
    "        return log_pxz.sum(dim=(1))\n",
    "    \n",
    "    \n",
    "    recon_loss = gaussian_likelihood(recon_x.view(-1,1,28,28), x.view(-1, 1,28,28))\n",
    "    kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return (recon_loss + kl_loss).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "5c5406a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, dataloader,epochs):\n",
    "    criterion = nn.MSELoss()\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for epoch in range(0,epochs): \n",
    "        print(f\"Epoch {epoch+1}\")\n",
    "        for i,data in enumerate(dataloader): \n",
    "            optimizer.zero_grad()\n",
    "            data =  torch.tensor(data, dtype = torch.float32)\n",
    "            reconstruction, mu, logvar = model(data)\n",
    "            loss = criterion(reconstruction,data)\n",
    "            #loss = vae_loss(reconstruction,data, mu, logvar)\n",
    "            loss.backward()\n",
    "            running_loss += loss.item()\n",
    "            optimizer.step()\n",
    "        train_loss = running_loss/len(dataloader.dataset)\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "3bd669d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_imgs, batch_size=16,\n",
    "                                         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de1ac2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "dadbf407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cd/hq134zy930x005dbg266m4wh0000gn/T/ipykernel_19302/978004405.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data =  torch.tensor(data, dtype = torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4115.3970\n",
      "Epoch 2\n",
      "Train Loss: 8230.7658\n",
      "Epoch 3\n",
      "Train Loss: 12346.1434\n",
      "Epoch 4\n",
      "Train Loss: 16461.6243\n",
      "Epoch 5\n",
      "Train Loss: 20577.0818\n",
      "Epoch 6\n",
      "Train Loss: 24692.4614\n",
      "Epoch 7\n",
      "Train Loss: 28807.8595\n",
      "Epoch 8\n",
      "Train Loss: 32923.2021\n",
      "Epoch 9\n",
      "Train Loss: 37038.5815\n",
      "Epoch 10\n",
      "Train Loss: 41153.9767\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "41153.97668850807"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(vae,train_dataloader,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "c65e1e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = vae.forward(torch.tensor(train_imgs[0], dtype = torch.float32).view((1,1,28,28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "b1af12b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 28, 28])"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "1f526209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9113a40e20>"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAKlklEQVR4nO3dT4ic9R3H8c+nm0ipesh/tjE0VtJSKTWWIRRSikXUmEv00GIOkoKwHhQUPFTsoR5DqUoPRVhrMC1WKaiYQ2gMQQhCsY6yzZ+mNamkzZoluyYHIz1o1m8P+6SMcWZ3nOd55hn8vl+wzMwzz87zZcg78zf5OSIE4MvvK00PAGA4iB1IgtiBJIgdSILYgSSWDfNgq1eOxcYNy4d5SGAg7x75Wq23/63v/beW2z195hN9cGHe3a4rFbvtbZJ+I2lM0u8iYvdi+2/csFx/PbChzCGBobjj65trvf0DB6Zqud0td5zped3AT+Ntj0n6raQ7Jd0oaaftGwe9PQD1KvOafYukUxHxXkR8LOlFSTuqGQtA1crEvl5S53OG6WLbZ9iesN223Z47P1/icADKKBN7tzcBPvfd24iYjIhWRLTWrBorcTgAZZSJfVpS57tt10k6W24cAHUpE/tbkjbZvt72VZLukbSvmrEAVG3gj94i4pLtByUd0MJHb3si4nhlkwENOnB2atHr6/5org6lPmePiP2S9lc0C4Aa8XVZIAliB5IgdiAJYgeSIHYgCWIHkhjqv2cHviyW+hx+FPHIDiRB7EASxA4kQexAEsQOJEHsQBLEDiRB7EASxA4kQexAEsQOJEHsQBLEDiRB7EASxA4kQexAEsQOJEHsQBLEDiRB7EASxA4kQexAEsQOJFHq/423fVrSRUnzki5FRKuKoQBUr4pFIn4cER9UcDsAasTTeCCJsrGHpNdsv217otsOtidst223587PlzwcgEGVfRq/NSLO2l4r6aDtf0TE4c4dImJS0qQktW76apQ8HoABlXpkj4izxemspFckbaliKADVGzh221fbvvbyeUm3SzpW1WAAqlXmafw6Sa/Yvnw7f4yIP1cyFYDKDRx7RLwn6aYKZwFQIz56A5IgdiAJYgeSIHYgCWIHkiB2IAliB5IgdiAJYgeSIHYgCWIHkiB2IAliB5IgdiAJYgeSIHYgCWIHkiB2IAliB5IgdiAJYgeSIHYgCWIHkiB2IAliB5IgdiAJYgeSIHYgCWIHkiB2IIklY7e9x/as7WMd21baPmj7ZHG6ot4xAZTVzyP7c5K2XbHtUUmHImKTpEPFZQAjbMnYI+KwpAtXbN4haW9xfq+ku6odC0DVBn3Nvi4iZiSpOF3ba0fbE7bbtttz5+cHPByAsmp/gy4iJiOiFRGtNavG6j4cgB4Gjf2c7XFJKk5nqxsJQB0GjX2fpF3F+V2SXq1mHAB16eejtxck/UXSt21P275P0m5Jt9k+Kem24jKAEbZsqR0iYmePq26teBYANeIbdEASxA4kQexAEsQOJEHsQBLEDiRB7EASxA4kQexAEsQOJEHsQBLEDiRB7EASxA4kQexAEsQOJEHsQBLEDiRB7EASxA4kQexAEsQOJEHsQBLEDiRB7EASxA4kQexAEsQOJEHsQBLEDiTRz/rse2zP2j7Wse1x2+/bnip+ttc7JoCy+nlkf07Sti7bn4qIzcXP/mrHAlC1JWOPiMOSLgxhFgA1KvOa/UHbR4qn+St67WR7wnbbdnvu/HyJwwEoY9DYn5Z0g6TNkmYkPdFrx4iYjIhWRLTWrBob8HAAyhoo9og4FxHzEfGppGckbal2LABVGyh22+MdF++WdKzXvgBGw7KldrD9gqRbJK22PS3pl5Jusb1ZUkg6Len++kYEUIUlY4+InV02P1vDLABqxDfogCSIHUiC2IEkiB1IgtiBJIgdSILYgSSIHUiC2IEkiB1IgtiBJIgdSILYgSSIHUiC2IEkiB1IgtiBJIgdSILYgSSIHUiC2IEkiB1IgtiBJIgdSILYgSSIHUiC2IEkiB1IgtiBJIgdSGLJ2G1vsP267RO2j9t+qNi+0vZB2yeL0xX1jwtgUP08sl+S9EhEfEfSDyQ9YPtGSY9KOhQRmyQdKi4DGFFLxh4RMxHxTnH+oqQTktZL2iFpb7HbXkl31TQjgAp8odfstjdKulnSm5LWRcSMtPAXgqS1PX5nwnbbdnvu/HzJcQEMqu/YbV8j6SVJD0fEh/3+XkRMRkQrIlprVo0NMiOACvQVu+3lWgj9+Yh4udh8zvZ4cf24pNl6RgRQhX7ejbekZyWdiIgnO67aJ2lXcX6XpFerHw9AVZb1sc9WSfdKOmp7qtj2mKTdkv5k+z5J/5H0k1omBFCJJWOPiDckucfVt1Y7DoC68A06IAliB5IgdiAJYgeSIHYgCWIHkiB2IAliB5IgdiAJYgeSIHYgCWIHkiB2IAliB5IgdiAJYgeSIHYgCWIHkiB2IAliB5IgdiAJYgeSIHYgCWIHkiB2IAliB5IgdiAJYgeSIHYgiX7WZ99g+3XbJ2wft/1Qsf1x2+/bnip+ttc/LoBB9bM++yVJj0TEO7avlfS27YPFdU9FxK/rGw9AVfpZn31G0kxx/qLtE5LW1z0YgGp9odfstjdKulnSm8WmB20fsb3H9ooevzNhu227PXd+vty0AAbWd+y2r5H0kqSHI+JDSU9LukHSZi088j/R7fciYjIiWhHRWrNqrPzEAAbSV+y2l2sh9Ocj4mVJiohzETEfEZ9KekbSlvrGBFBWP+/GW9Kzkk5ExJMd28c7drtb0rHqxwNQlX7ejd8q6V5JR21PFdsek7TT9mZJIem0pPtrmA9ARfp5N/4NSe5y1f7qxwFQF75BByRB7EASxA4kQexAEsQOJEHsQBLEDiRB7EASxA4kQexAEsQOJEHsQBLEDiRB7EASjojhHcyek/Tvjk2rJX0wtAG+mFGdbVTnkphtUFXO9o2IWNPtiqHG/rmD2+2IaDU2wCJGdbZRnUtitkENazaexgNJEDuQRNOxTzZ8/MWM6myjOpfEbIMaymyNvmYHMDxNP7IDGBJiB5JoJHbb22z/0/Yp2482MUMvtk/bPlosQ91ueJY9tmdtH+vYttL2Qdsni9Oua+w1NNtILOO9yDLjjd53TS9/PvTX7LbHJL0r6TZJ05LekrQzIv4+1EF6sH1aUisiGv8Chu0fSfpI0u8j4rvFtl9JuhARu4u/KFdExM9HZLbHJX3U9DLexWpF453LjEu6S9LP1OB9t8hcP9UQ7rcmHtm3SDoVEe9FxMeSXpS0o4E5Rl5EHJZ04YrNOyTtLc7v1cIflqHrMdtIiIiZiHinOH9R0uVlxhu97xaZayiaiH29pDMdl6c1Wuu9h6TXbL9te6LpYbpYFxEz0sIfHklrG57nSksu4z1MVywzPjL33SDLn5fVROzdlpIapc//tkbE9yXdKemB4ukq+tPXMt7D0mWZ8ZEw6PLnZTUR+7SkDR2Xr5N0toE5uoqIs8XprKRXNHpLUZ+7vIJucTrb8Dz/N0rLeHdbZlwjcN81ufx5E7G/JWmT7ettXyXpHkn7Gpjjc2xfXbxxIttXS7pdo7cU9T5Ju4rzuyS92uAsnzEqy3j3WmZcDd93jS9/HhFD/5G0XQvvyP9L0i+amKHHXN+U9Lfi53jTs0l6QQtP6z7RwjOi+yStknRI0snidOUIzfYHSUclHdFCWOMNzfZDLbw0PCJpqvjZ3vR9t8hcQ7nf+LoskATfoAOSIHYgCWIHkiB2IAliB5IgdiAJYgeS+B+nuE8CTyLmjQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(res[0][0][0].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "33f62198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0][0][0].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1727d8c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3686788c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5c58ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
